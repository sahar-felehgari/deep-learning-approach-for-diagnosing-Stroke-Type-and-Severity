# -*- coding: utf-8 -*-
"""deep learning approach for diagnosing Stroke Type and Severity .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n0G8Ns_AZPYiw6zJ-MuVW1BLgjWirdWP

**1.preprocessing**
"""

! pip install pydicom

! pip install normalization

import numpy as np
import keras
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Dense, Conv2D, BatchNormalization, Activation
from keras.layers import AveragePooling2D, Input, Flatten
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model
import numpy as np
from keras import regularizers
from tensorflow.keras import initializers
#from keras.layers.normalization import BatchNormalization
from keras.applications import ResNet152V2
import os
import tensorflow as tf
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.preprocessing.image import ImageDataGenerator
import os
import zipfile
import tensorflow as tf
import numpy as np
import keras
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Dense, Conv2D, BatchNormalization, Activation
from keras.layers import AveragePooling2D, Input, Flatten
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras.regularizers import l2
from keras import backend as K
from keras.models import Model
import numpy as np
import os
import tensorflow as tf
from keras.optimizers import SGD, Adam
#from keras.utils.np_utils import to_categorical
#from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential
from keras.layers import Dense
from keras.regularizers import l2
from keras.models import Sequential
from keras.layers import Conv2D, TimeDistributed,Lambda, MaxPooling2D,AveragePooling2D, GRU# convolution layers
from keras.layers import Dense, Dropout, Flatten,Activation,Reshape,Bidirectional # core layers1
#from keras.layers.normalization import BatchNormalization
from keras.applications.resnet50 import ResNet50
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
import numpy as np

import tensorflow as tf
import os
import cv2
import random
import numpy as np
import pandas as pd
import pydicom
import keras
import matplotlib.pyplot as plt
from skimage import transform
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import to_categorical
import tensorflow as tf
from keras import layers
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support
from scipy import ndimage
from skimage import io, exposure, img_as_float
from sklearn import svm
from sklearn.metrics import classification_report
from tensorflow.keras import layers
from tensorflow.keras.preprocessing import image

!pip install rarfile
!pip install patool

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set the path to your .dcm  image folder
img_dir = "/content/drive/MyDrive/Stroke Detection/Tracew+Adc+Normal/unzip-Tracew+Adc+Normal/Tracew+Adc+Normal"
csv_path = "/content/drive/MyDrive/Stroke Detection/Tracew+Adc+Normal/tracew+adc+normal(ischem+hemorr+normal).csv"

# Set the target image size
imge_size = (224, 224)

# Load the DWIlabels from the CSV file into a pandas DataFrame
labels_df = pd.read_csv(csv_path)

labels_df

# Show pixel image size
dcmHandler = pydicom.dcmread("/content/drive/MyDrive/Stroke Detection/Tracew+Adc+Normal/unzip-Tracew+Adc+Normal/Tracew+Adc+Normal/ADC-ischemic (127).dcm")
print("Pixel Image Size: ", dcmHandler.pixel_array.shape)

dcm_ext = '.dcm'

#function for Load the images and preprocessing them
def load_images(filename):
    # Load the ADC image
    img_file = os.path.join(img_dir, filename + dcm_ext)
    img_dcm = pydicom.dcmread(img_file)
    image = img_dcm.pixel_array
    image = cv2.resize(image, (224, 224))

  #    #PREPROCCESING
  #    # Normalize the DWI data using exposure equalization
    img_data = exposure.equalize_hist(image)

  #  # Rescale the image data between 0 and 1
    img_data = img_as_float(img_data)


    image = np.dstack([img_data, img_data,img_data])
    image =  cv2.normalize(src=image, dst=image, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)
    # # display grayscale image
    # plt.imshow(image, cmap='gray')
    # plt.axis('off')
    # plt.show()
    return image

# Load the images and labels into numpy arrays
X = []
y = []
for filename, img_label in zip(labels_df['ID'], labels_df['label']):
    image = load_images(filename)
    if image is not None:
        X.append(image)
        if img_label == 'normal':
            y.append(0)
        elif img_label == 'ischemic':
             y.append(1)
        elif img_label == 'hemorrhagic':
             y.append(2)

# Convert the lists to numpy arrays
X = np.stack(X)
y = np.array(y)

len_x= len(X)
len_y= len(X)
print("len_y:",len_y,"len_x:",len_x)

print(X[1].shape)

y= to_categorical(y,num_classes=3)
#print(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)

# Split the data into training and testing sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

print(y_train.shape)
print(y_test.shape)

print(X_train.shape)
print(X_test.shape)

print(X_val.shape)
print(y_val.shape)

from tensorflow import keras
from tensorflow.keras import layers
from keras.preprocessing import image

data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomFlip("vertical"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2),
        layers.RandomContrast(0.2),

    ]
)

# Create an image data generator object to apply data augmentation
train_datagen = ImageDataGenerator(
    preprocessing_function=data_augmentation,
    validation_split=0.2
)

# Create the training data generator
train_generator = train_datagen.flow(
    x=X_train,
    y=y_train,
    batch_size=32,
    shuffle=True,
    subset='training'
)

# Create the validation data generator
val_generator = train_datagen.flow(
    x=X_train,
    y=y_train,
    batch_size=32,
    shuffle=True,
    subset='validation'
)

"""# **NON- combine- stroke detection**

**ResNet-single modal**
"""

# Create the ResNet model
base_model = keras.applications.ResNet50(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_resnet = keras.Model(inputs, outputs)

# Compile the model
model_resnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model_resnet.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_val, y_val),
                    epochs=150, verbose=1,callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy-resnet")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss-resnet")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_resnet.predict(X_test)

#Save the model to disk
model_resnet.save("my_model.h6")

#Predict on test data
test_loss, test_acc= model_resnet.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for Resnet-NONcombined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-resnet-NONcombine {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_resnet.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve_resnet (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**MobileNet/single modal**"""

from tensorflow.keras.applications import MobileNet
MobileNet = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))
for layer in MobileNet.layers:
  layer.trainable= False

# Create the ResNet model
base_model = keras.applications.MobileNet(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_MobileNet2 = keras.Model(inputs, outputs)

# Compile the model
model_MobileNet2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model_MobileNet2.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_val, y_val),
                    epochs=150, verbose=1,callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_MobileNet2.predict(X_test)

#Save the model to disk
model_MobileNet.save("my_model.h7")

#Predict on test data
test_loss, test_acc= model_MobileNet2.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for MobileNet-NONcombined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-MobileNet-NONcombine {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_MobileNet2.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**ACl-MobileNet/single modal**"""

from tensorflow.keras.applications import MobileNet
MobileNet = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))
for layer in MobileNet.layers:
  layer.trainable= False

inputs = keras.Input(shape=(256, 256, 3))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(3, activation="softmax")(x)
model_MobileNet = keras.Model(inputs=inputs, outputs=outputs)

inputs = keras.Input(shape=(256, 256, 3))
x = data_augmentation(inputs)
x = keras.applications.mobilenet.preprocess_input(x)
x = keras.applications.MobileNet(include_top=False, input_shape=(256, 256, 3))(x)
x = layers.Flatten()(x)
x = layers.Dense(256)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(3, activation="softmax")(x)
model_MobileNet = keras.Model(inputs, outputs)
model_MobileNet.compile(loss="categorical_crossentropy",
    optimizer="rmsprop",
    metrics=["accuracy"])

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

model_MobileNet.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_MobileNet.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_val, y_val),
                    epochs=150, verbose=1,callbacks=callbacks)

#Save the model to disk
model_MobileNet.save2("my_model.h5")

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_MobileNet.predict(X_test)

#Predict on test data
test_loss, test_acc= model_MobileNet.evaluate(X_test, y_test, verbose=2)
#Save the model to disk
#model_MobileNet.save2("my_model.h5")

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for MobileNet-NONcombined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-MobileNet-NONcombine {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_MobileNet.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**ACD-ResNet-single modal**"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Create the ResNet model
base_model = keras.applications.ResNet50(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Flatten()(x)
outputs = layers.Dense(3, activation="softmax")(x)

model_resnet_morelayers = keras.Model(inputs=inputs, outputs=outputs)
model_resnet_morelayers.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_resnet_morelayers.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_test, y_test),
                    epochs=100, verbose=1,callbacks=callbacks)

model_resnet_morelayers.summary()

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy-VGG16")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss-VGG16")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_resnet_morelayers.predict(X_test)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for VGG16-NONcombined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-VGG16-NONcombine {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_resnet_morelayers.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""# **(DWI and ADC combined)**"""

# Set the path to your .dcm DWI image folder
dwi_dir = "/content/drive/MyDrive/Stroke Detection/unzip-dwi stroke detection/dwi for detection stroke"
csvDWI_path = "/content/drive/MyDrive/Stroke Detection/detection type of stroke.csv"

# Set the path to your .dcm ADC image folder
adc_dir = "//content/drive/MyDrive/Stroke Detection/unzip-adc stroke detection/adc for detection stroke"
csvADC_path = "/content/drive/MyDrive/Stroke Detection/detection type of stroke.csv"

# Set the target image size
imge_size = (256, 256)

# Load the DWIlabels from the CSV file into a pandas DataFrame
dwi_labels_df = pd.read_csv(csvDWI_path)
# Load the ADClabels from the CSV file into a pandas DataFrame
adc_labels_df = pd.read_csv(csvADC_path)

dwi_labels_df

adc_labels_df

# Merge the DWI and ADC labels based on the filenames
labels_df = pd.merge(dwi_labels_df, adc_labels_df, on="ID")

labels_df

dcmHandler = pydicom.dcmread("/content/drive/MyDrive/Untitled folder/Stroke Detection/data testi/ADC/Untitled folder/normal (3).dcm")
# Show pixel image size
print("Pixel Image Size-ADC: ", dcmHandler.pixel_array.shape)
dcmHandler = pydicom.dcmread("/content/drive/MyDrive/Untitled folder/Stroke Detection/data testi/DWI-Tracew/Untitled folder/normal (3).dcm")
# Show pixel image size
print("Pixel Image Size-DWI: ", dcmHandler.pixel_array.shape)

dwi_ext = '.dcm'
adc_ext = '.dcm'

import matplotlib.pyplot as plt
import numpy as np
import pydicom
from skimage.color import rgb2gray

# set paths to ADC and DWI DICOM files
adc_path = '/content/drive/MyDrive/Untitled folder/Stroke Detection/unzip-adc stroke detection/adc for detection stroke/90 (15).dcm'
dwi_path = '/content/drive/MyDrive/Untitled folder/Stroke Detection/unzip-dwi stroke detection/dwi for detection stroke/90 (15).dcm'

# read in DICOM files
adc = pydicom.read_file(adc_path).pixel_array
dwi = pydicom.read_file(dwi_path).pixel_array
adc = cv2.resize(adc, (256, 256))
dwi = cv2.resize(dwi, (256, 256))
# combine ADC and DWI into a single 3D array
combined = np.stack([adc, dwi], axis=-1)
gray = np.min(combined, axis=-1)
X = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
# convert to grayscale
print(X.shape)
image =  cv2.normalize(src=X, dst=X, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

plt.imshow(adc, cmap='gray')
plt.axis('off')
plt.show()
plt.imshow(dwi, cmap='gray')
plt.axis('off')
plt.show()

# display grayscale image
plt.imshow(X, cmap='gray')
plt.axis('off')
plt.show()

# display grayscale image
plt.imshow(image, cmap='gray')
plt.axis('off')
plt.show()

image.shape

#function for Load the images and preprocessing them
def load_images_ischemic(filename):
    # Load the ADC image
    adc_file = os.path.join(adc_dir, filename + adc_ext)
    adc = pydicom.read_file(adc_file).pixel_array


    # Load the DWI image
    dwi_file = os.path.join(dwi_dir, filename + dwi_ext)
    dwi = pydicom.read_file(dwi_file).pixel_array
    adc = cv2.resize(adc, (256, 256))
    dwi = cv2.resize(dwi, (256, 256))

    combined = np.stack([adc, dwi], axis=-1)
    gray = np.min(combined, axis=-1)
    X = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
    # convert to grayscale
    #print(X.shape)
    image =  cv2.normalize(src=X, dst=X, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

    #print(image.shape)
    return image

labels_df = labels_df.rename(columns={'Label _x': 'Label_x', 'Label _y': 'Label_y'})

# Load the images and labels into numpy arrays
X_combine = []
y_combine = []
for filename, dwi_label, adc_label in zip(labels_df['ID'], labels_df['Label_x'], labels_df['Label_y']):
      image = load_images_ischemic(filename)
      if image is not None:
         X_combine.append(image)
         y_combine.append(1)

len_X_combine= len(X_combine)
len_y_combine= len(y_combine)
print("len_y_combine:",len_y_combine,"len_X_combine:",len_X_combine)

#Normal images path
NORMAL_IMAGE="/content/drive/MyDrive/Stroke Detection/all normal images"
CSV_NORMAL="/content/drive/MyDrive/Stroke Detection/all normal images/normal images.csv"

# Show pixel image size
dcmHandler = pydicom.dcmread("/content/drive/MyDrive/Stroke Detection/all normal images/ADC-normal (1).dcm")
print("Pixel Image Size-normal: ", dcmHandler.pixel_array.shape)

# read in DICOM files
import numpy as np
from PIL import Image

bb = '/content/drive/MyDrive/Stroke Detection/all normal images/ADC-normal (3).dcm'
nr = pydicom.read_file(bb).pixel_array
norm = cv2.resize(nr, (256, 256))
# combine ADC and DWI into a single 3D array
combined = np.stack([norm, norm], axis=-1)
gray = np.min(combined, axis=-1)
X = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
# convert to grayscale
print(X.shape)
image =  cv2.normalize(src=X, dst=X, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

#print(image_normal.shape)
# Convert to PIL Image and save as PNG
plt.imshow(nr,cmap='gray')
plt.axis('off')
plt.show()

# display grayscale image
plt.imshow(image, cmap='gray')
plt.axis('off')
plt.show()
image.shape

import numpy as np
from PIL import Image

bb = '/content/drive/MyDrive/Untitled folder/Stroke Detection/all normal images/ADC-normal (3).dcm'
nr = pydicom.read_file(bb).pixel_array
norm = cv2.resize(nr, (256, 256))
# Normalize the DWI data using exposure equalization
normal_data = exposure.equalize_hist(norm)

# Rescale the image data between 0 and 1
normal_data = img_as_float(normal_data)

# Combine the ADC and DWI images into a single 3D array
image = np.stack([dwi_image, adc_image], axis=-1)
image =  cv2.normalize(src=X, dst=X, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

print(image_normal.shape)
# Convert to PIL Image and save as PNG
plt.imshow(norm,cmap='gray')
plt.axis('off')
plt.show()

# display grayscale image
plt.imshow(image, cmap='gray')
plt.axis('off')
plt.show()
image.shape

NORMAL_labels_df = pd.read_csv(CSV_NORMAL)

NORMAL_labels_df

#function for Load the images and preprocessing them
def load_images_normal2(filename):
    # Load  image
    normal_file = os.path.join(NORMAL_IMAGE, filename + adc_ext)
    normal_dcm = pydicom.dcmread(normal_file)
    normal_image = normal_dcm.pixel_array
    normal_image = cv2.resize(normal_image, (256, 256))


     #PREPROCCESING
     # Normalize the DWI data using exposure equalization
    #normal_data = exposure.equalize_hist(normal_image)

   # Rescale the image data between 0 and 1
    #normal_data = img_as_float(normal_data)

    # Combine the ADC and DWI images into a single 3D array
    #image = np.stack([dwi_image, adc_image], axis=-1)
    image_normal = np.stack([normal_image, normal_image], axis=-1)
    gray = np.min(image_normal, axis=-1)
    X = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
    image =  cv2.normalize(src=X, dst=X, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

    return image

# Load the images and labels into numpy arrays

for filename, notmal_label in zip(NORMAL_labels_df['ID'], NORMAL_labels_df['lable']):
    image_normal = load_images_normal2(filename)
    if image is not None:
      # Apply alignment to the images
        #image = transform.rotate(image, -90, resize=True)
        X_combine.append(image)
        #if dwi_label == 'stroke' or adc_label == 'stroke':
        y_combine.append(0)
        #else:
            #y.append(0)

len_x= len(X_combine)
len_y= len(y_combine)
print("len_y_combine:",len_y,"len_x_combine:",len_x)

"""**Load Hemorrahgic Images**"""

#hemorrahgic images path
hemorrahgic_IMAGE="/content/drive/MyDrive/Stroke Detection/hemorragic/unzip-hemorragic/all slices-finall(TRACEW only)"
CSV_hemorrahgic="/content/drive/MyDrive/Stroke Detection/hemorragic/hemorrhagic-finall(TRACEW only).csv"

# Show pixel image size
dcmHandler = pydicom.dcmread("/content/drive/MyDrive/Stroke Detection/hemorragic/unzip-hemorragic/all slices-finall(TRACEW only)/h (1).dcm")
print("Pixel Image Size-hemorrhagic: ", dcmHandler.pixel_array.shape)

hemorrahgic_labels_df = pd.read_csv(CSV_hemorrahgic)

hemorrahgic_labels_df = hemorrahgic_labels_df.drop('Unnamed: 2', axis=1)

hemorrahgic_labels_df

def load_images_hemorr2(filename):
    # Load image
    hemorr_file = os.path.join(hemorrahgic_IMAGE, filename + adc_ext)
    hemorr_dcm = pydicom.dcmread(hemorr_file)
    hemorr_image = hemorr_dcm.pixel_array
    hemorr_image = cv2.resize(hemorr_image, (256, 256))

    image_hemorr = np.stack([hemorr_image, hemorr_image], axis=-1)
    gray = np.min(image_hemorr, axis=-1)
    X = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
    image =  cv2.normalize(src=X, dst=X, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

    return image

# Load the images and labels into numpy arrays

for filename, hemorr_label in zip(hemorrahgic_labels_df['ID'], hemorrahgic_labels_df['lable']):
    image = load_images_hemorr2(filename)
    if image is not None:
      # Apply alignment to the images
        #image = transform.rotate(image, -90, resize=True)
        X_combine.append(image)
        #if dwi_label == 'stroke' or adc_label == 'stroke':
        y_combine.append(2)
        #else:
            #y.append(0)

len_x= len(X_combine)
len_y= len(y_combine)
print("len_y_combine:",len_y,"len_x_combine:",len_x)

# Convert the image data to a numpy array
X_combine = np.array(X_combine)

# Convert labels to categorical
y_combine = to_categorical(y_combine, num_classes=3)

# Print shapes of X and y
print("X shape:", X_combine.shape)
print("y shape:", y_combine.shape)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_combine, y_combine, test_size=0.2, random_state=42, stratify=y_combine)

print(y_train.shape)
print(y_test.shape)

print(X_train.shape)
print(X_test.shape)

from tensorflow import keras
from tensorflow.keras import layers
from keras.preprocessing import image

data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomFlip("vertical"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2),
        layers.RandomBrightness(0.3),
        layers.RandomContrast(0.2),
        #layers.RandomTranslation(height_factor=0.1, width_factor=0.1),
        #layers.RandomRotation(factor=0.2),
        #layers.GaussianNoise(stddev=0.01)
    ]
)

# Create an image data generator object to apply data augmentation
train_datagen = ImageDataGenerator(
    preprocessing_function=data_augmentation,
    validation_split=0.2
)

# Create the training data generator
train_generator = train_datagen.flow(
    x=X_train,
    y=y_train,
    batch_size=32,
    shuffle=True,
    subset='training'
)

# Create the validation data generator
val_generator = train_datagen.flow(
    x=X_train,
    y=y_train,
    batch_size=32,
    shuffle=True,
    subset='validation'
)

"""**ACL-Mobilnet/multimodal**"""

from tensorflow.keras.applications import MobileNet
MobileNet = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))
for layer in MobileNet.layers:
  layer.trainable= False

inputs = keras.Input(shape=(256, 256, 3))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(3, activation="softmax")(x)
model_MobileNet = keras.Model(inputs=inputs, outputs=outputs)

inputs = keras.Input(shape=(256, 256, 3))
x = data_augmentation(inputs)
x = keras.applications.mobilenet.preprocess_input(x)
x = keras.applications.MobileNet(include_top=False, input_shape=(256, 256, 3))(x)
x = layers.Flatten()(x)
x = layers.Dense(256)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(3, activation="softmax")(x)
model_MobileNet = keras.Model(inputs, outputs)
model_MobileNet.compile(loss="categorical_crossentropy",
    optimizer="rmsprop",
    metrics=["accuracy"])

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

model_MobileNet.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_MobileNet.fit(train_generator,
                    batch_size=32,
                    validation_data=val_generator,
                    epochs=100, verbose=1,callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy-model_MobileNet")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss-model_MobileNet")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_MobileNet.predict(X_test)

#Save the model to disk
model_MobileNet.save("my_model.h12")

#Predict on test data
test_loss, test_acc= model_MobileNet.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for model_MobileNet-combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-model_MobileNet-combined {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_MobileNet_nolayer.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**Mobilnet/multimodal**"""

from tensorflow.keras.applications import MobileNet
MobileNet = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))
for layer in MobileNet.layers:
  layer.trainable= False

# Create the ResNet model
base_model = keras.applications.MobileNet(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_MobileNet_nolayer = keras.Model(inputs, outputs)

# Compile the model
model_MobileNet_nolayer.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_MobileNet_nolayer.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history_mob = model_MobileNet_nolayer.fit(train_generator,
                    batch_size=32,
                    validation_data=val_generator,
                    epochs=100, verbose=1,callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history_mob.history["accuracy"]
val_accuracy = history_mob.history["val_accuracy"]
loss = history_mob.history["loss"]
val_loss = history_mob.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_MobileNet_nolayer.predict(X_test)

#Save the model to disk
model_MobileNet_nolayer.save("my_model.h16")

#Predict on test data
test_loss, test_acc= model_MobileNet_nolayer.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for MobileNet-NONcombined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-MobileNet-NONcombine {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_MobileNet_nolayer.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**Resnet/multimodal**"""

# Create the ResNet model
base_model = keras.applications.ResNet50(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_resnet_combine = keras.Model(inputs, outputs)

# Compile the model
model_resnet_combine.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_resnet_combine.summary()

# Train the model
history = model_resnet_combine.fit(
       train_generator,
       epochs=100,
      validation_data=val_generator,
)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy-resnet")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss-resnet")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_resnet_combine.predict(X_test)

#Save the model to disk
model_resnet_combine.save("my_model.h13")

#Predict on test data
test_loss, test_acc= model_resnet_combine.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for Resnet-combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-resnet-combine {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_resnet_combine.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve_resnet (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**ACL-Resnet/multimodal**"""

import tensorflow as tf
from tensorflow import keras

# Create the ResNet model
base_model = keras.applications.ResNet50(include_top=False, weights="imagenet", input_shape=(256, 256, 3))
# Freeze the base model
base_model.trainable = False

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Create the ResNet model
base_model = keras.applications.ResNet50(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(3, activation="softmax")(x)

model_resnet_morelayers1= keras.Model(inputs=inputs, outputs=outputs)
model_resnet_morelayers1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_resnet_morelayers1.fit(train_generator,
                    batch_size=32,
                    validation_data=val_generator,
                    epochs=100, verbose=1,callbacks=callbacks)

model_resnet_morelayers1.summary()

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_resnet_morelayers1.predict(X_test)

#Predict on test data
test_loss, test_acc= model_resnet_morelayers1.evaluate(X_test, y_test, verbose=2)
#Save the model to disk
model_resnet_morelayers1.save("my_model.h13")

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for ResNet-combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_resnet_morelayers1.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""# **VGG16-Multimodal**"""

# Freeze the base model
vgg16.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = vgg16(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_vgg16 = keras.Model(inputs, outputs)

# Compile the model
model_vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model_vgg16.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_test, y_test),
                    epochs=100, verbose=1)



import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_vgg16.predict(X_test)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for model_vgg16_combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for model_vgg16_combined {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_vgg16.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**Inception V3- moltimudal**"""

#from tensorflow.keras.applications import MobileNet
InceptionV3 =keras.applications.InceptionV3(include_top=False, weights="imagenet", input_shape=(256, 256, 3))
for layer in InceptionV3.layers:
  layer.trainable= False

InceptionV3.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = InceptionV3(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_InceptionV3 = keras.Model(inputs, outputs)

# Compile the model
model_InceptionV3.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model_InceptionV3.summary()

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_InceptionV3.fit(train_generator,
                    batch_size=32,
                    validation_data=val_generator,
                    epochs=100, verbose=1,callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

history = model_InceptionV3.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_test, y_test),
                    epochs=100, verbose=1,)#callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_InceptionV3.predict(X_test)

#Predict on test data
test_loss, test_acc= model_InceptionV3.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for model_InceptionV3_combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for model_InceptionV3_combined {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using model_InceptionV3

# Make predictions on the testing data
y_pred_prob = model_InceptionV3.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

#Save the model to disk
model_InceptionV3.save("gdrive/My Drive/my_model.InceptionV3_multimodal")

"""**Inception V3- single modal**

Model
"""

#from tensorflow.keras.applications import MobileNet
InceptionV3_single =keras.applications.InceptionV3(include_top=False, weights="imagenet", input_shape=(224, 224, 3))
for layer in InceptionV3_single.layers:
  layer.trainable= False

InceptionV3_single.trainable = False

# Add the classification head
inputs = keras.Input(shape=(224, 224, 3))
x = InceptionV3_single(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_InceptionV3_single = keras.Model(inputs, outputs)

# Compile the model
model_InceptionV3_single.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model_InceptionV3_single.summary()

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_InceptionV3_single.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_test, y_test),
                    epochs=100, verbose=1,)#callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_InceptionV3_single.predict(X_test)

#Predict on test data
test_loss, test_acc= model_InceptionV3_single.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for model_InceptionV3_single_combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for model_InceptionV3_single {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using model_InceptionV3

# Make predictions on the testing data
y_pred_prob = model_InceptionV3_single.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

#Save the model to disk
model_InceptionV3_single.save("gdrive/My Drive/my_model.InceptionV3_single")

"""**DensNet-Single**"""

#from tensorflow.keras.applications import MobileNet
DenseNet169_single =keras.applications.DenseNet169(include_top=False, weights="imagenet", input_shape=(224, 224, 3))
for layer in DenseNet169_single.layers:
  layer.trainable= False

DenseNet169_single.trainable = False

# Add the classification head
inputs = keras.Input(shape=(224, 224, 3))
x = DenseNet169_Multi(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_DenseNet169_single = keras.Model(inputs, outputs)

# Compile the model
modelDenseNet169_single.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model_DenseNet169_single.summary()

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_DenseNet169_single.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_test, y_test),
                    epochs=100, verbose=1,)#callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_DenseNet169_single.predict(X_test)

#Predict on test data
test_loss, test_acc= model_DenseNet169_single.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for model_DenseNet169_single")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for model_DenseNet169_single {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using model_InceptionV3

# Make predictions on the testing data
y_pred_prob = model_DenseNet169_single.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

#Save the model to disk
model_DenseNet169_single.save("gdrive/My Drive/my_model.DenseNet169_single")

"""**DensNet-Multi**"""

#from tensorflow.keras.applications import MobileNet
DenseNet169_Multi =keras.applications.DenseNet169(include_top=False, weights="imagenet", input_shape=(256, 256, 3))
for layer in DenseNet169_Multi.layers:
  layer.trainable= False

DenseNet169_Multi.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = DenseNet169_Multi(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_DenseNet169_Multi = keras.Model(inputs, outputs)

# Compile the model
model_DenseNet169_Multi.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model_DenseNet169_Multi.summary()

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_DenseNet169_Multi.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_test, y_test),
                    epochs=100, verbose=1,)#callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_DenseNet169_Multi.predict(X_test)

#Predict on test data
test_loss, test_acc= model_DenseNet169_Multi.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for model_model_DenseNet169_Multi")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for model_DenseNet169_Multi {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using model_InceptionV3

# Make predictions on the testing data
y_pred_prob = model_DenseNet169_Multi.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

#Save the model to disk
model_DenseNet169_Multi.save("gdrive/My Drive/my_model.DenseNet169_Multi")

"""**Efficient-Multi**"""

#from tensorflow.keras.applications import MobileNet
EfficientNetB6_Multi =keras.applications.EfficientNetB6(include_top=False, weights="imagenet", input_shape=(256, 256, 3))
for layer in EfficientNetB6_Multi.layers:
  layer.trainable= False

EfficientNetB6_Multi.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = EfficientNetB6_Multi(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_EfficientNetB6_Multi = keras.Model(inputs, outputs)

# Compile the model
model_EfficientNetB6_Multi.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model_EfficientNetB6_Multi.summary()

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_EfficientNetB6_Multi.fit(x=X_train,y=y_train,
                    batch_size=32,
                    validation_data=(X_test, y_test),
                    epochs=100, verbose=1,)#callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_EfficientNetB6_Multi.predict(X_test)

#Predict on test data
test_loss, test_acc= model_EfficientNetB6_Multi.evaluate(X_test, y_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for model_EfficientNetB6_Multi")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for EfficientNetB6_Multi {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using model_InceptionV3

# Make predictions on the testing data
y_pred_prob = model_EfficientNetB6_Multi.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

#Save the model to disk
model_EfficientNetB6_Multi.save("gdrive/My Drive/my_model.EfficientNetB6_Multi")



"""**Efficient-Single**"""

#from tensorflow.keras.applications import MobileNet
Efficient_Single =keras.applications.EfficientNetB6(include_top=False, weights="imagenet", input_shape=(224, 224, 3))
for layer in Efficient_Single.layers:
  layer.trainable= False

Efficient_Single.trainable = False

# Add the classification head
inputs = keras.Input(shape=(224, 224, 3))
x = Efficient_Single(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_Efficient_Single = keras.Model(inputs, outputs)

# Compile the model
model_Efficient_Single.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model_Efficient_Single.summary()

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

history = model_Efficient_Single.fit(x=X_s_train,y=y_s_train,
                    batch_size=32,
                    validation_data=(X_s_test, y_s_test),
                    epochs=100, verbose=1,)#callbacks=callbacks)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_Efficient_Single.predict(X_s_test)

#Predict on test data
test_loss, test_acc= model_Efficient_Single.evaluate(X_s_test, y_s_test, verbose=2)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for model_Efficient_Single")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_s_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_s_test[:, i].sum())
    plt.title("Confusion Matrix for model_Efficient_Single {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using model_Efficient_Single

# Make predictions on the testing data
y_pred_prob = model_Efficient_Single.predict(X_s_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_s_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_s_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_s_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_s_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

#Save the model to disk
model_Efficient_Single.save("gdrive/My Drive/my_model.Efficient_Single")



"""# NIHSS

# preprocessing
"""

import rarfile
with rarfile.RarFile('/content/drive/MyDrive/Stroke Detection/main data-stroke images/tracew/resnet-all-just Only slices containing strokes.rar', 'r') as archive:
    archive.extractall('/content/drive/MyDrive/Stroke Detection/Untitled Folder')

# Set the path to dcm DWI image folder
dwi_dir_nihss = "/content/drive/MyDrive/Untitled folder/Stroke Detection/Untitled Folder/new"
dwi_dir_nihss_csv ='/content/drive/MyDrive/Untitled folder/Stroke Detection/main data-stroke images/tracew/balanced data-for resnet-just entry nihss.csv'

# Load the DWIlabels from the CSV file into a pandas DataFrame
dwinihss_labels_df = pd.read_csv(dwi_dir_nihss_csv)

dwinihss_labels_df

dwinihss_labels_df = dwinihss_labels_df.rename(columns={'entry NIHSS range': 'label'})

dwinihss_labels_df

mask = dwinihss_labels_df.isna().any(axis=1)
num_missing_rows = mask.sum()

print("Number of rows with missing values:", num_missing_rows)

dwi_ext='.dcm'

# read in DICOM files
import numpy as np
from PIL import Image

bb = '/content/drive/MyDrive/Stroke Detection/Untitled Folder/new/111 (2).dcm'
nr = pydicom.read_file(bb).pixel_array
norm = cv2.resize(nr, (256, 256))
# combine ADC and DWI into a single 3D array
combined = np.stack([norm, norm], axis=-1)
gray = np.min(combined, axis=-1)
X = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
# convert to grayscale
print(X.shape)
image =  cv2.normalize(src=X, dst=X, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

#print(image_normal.shape)
# Convert to PIL Image and save as PNG
plt.imshow(nr,cmap='gray')
plt.axis('off')
plt.show()

# display grayscale image
plt.imshow(image, cmap='gray')
plt.axis('off')
plt.show()
image.shape

adc_ext=".dcm"

def load_images_NIHSS(filename):
    # Load image
    img_file = os.path.join(dwi_dir_nihss, filename + adc_ext)
    img_dcm = pydicom.dcmread(img_file)
    img_image = img_dcm.pixel_array
    img_image = cv2.resize(img_image, (256, 256))

    image = np.stack([img_image, img_image], axis=-1)
    gray = np.min(image, axis=-1)
    X = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
    image =  cv2.normalize(src=X, dst=X, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)

    return image

# Load the images and labels into numpy arrays
X = []
y = []
for filename, label in zip(dwinihss_labels_df['ID'], dwinihss_labels_df['label']):
    image = load_images_NIHSS(filename)
    if image is not None:

        X.append(image)

        if 'NIHSS= 1 to 4' in str(label):
             y.append(0)
        elif 'NIHSS= 5 to 15' in str(label):
             y.append(1)
        elif 'NIHSS= 16 to 20' in str(label):
             y.append(2)

# Convert the lists to numpy arrays
X = np.stack(X)
y = np.array(y)

len_x= len(X)
len_y= len(y)
print("len_y:",len_y,"len_x:",len_x)

y= to_categorical(y,num_classes=3)
#print(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Split the data into training and testing sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomFlip("vertical"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2),
        layers.RandomBrightness(0.3),
        layers.RandomContrast(0.2),
        #layers.RandomTranslation(height_factor=0.1, width_factor=0.1),
        #layers.RandomRotation(factor=0.2),
        #layers.GaussianNoise(stddev=0.01)
    ]
)

# Create an image data generator object to apply data augmentation
train_datagen = ImageDataGenerator(
    preprocessing_function=data_augmentation,
    validation_split=0.2
)

# Create the training data generator
train_generator = train_datagen.flow(
    x=X_train,
    y=y_train,
    batch_size=32,
    shuffle=True,
    subset='training'
)

# Create the validation data generator
val_generator = train_datagen.flow(
    x=X_train,
    y=y_train,
    batch_size=32,
    shuffle=True,
    subset='validation'
)

"""**ACL- MobileNetV1**"""

from tensorflow.keras.applications import MobileNet
MobileNet = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))
for layer in MobileNet.layers:
  layer.trainable= False

inputs = keras.Input(shape=(256, 256, 3))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(3, activation="softmax")(x)
model_MobileNet = keras.Model(inputs=inputs, outputs=outputs)

inputs = keras.Input(shape=(256, 256, 3))
x = data_augmentation(inputs)
x = keras.applications.mobilenet.preprocess_input(x)
x = keras.applications.MobileNet(include_top=False, input_shape=(256, 256, 3))(x)
x = layers.Flatten()(x)
x = layers.Dense(256)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(3, activation="softmax")(x)
model_MobileNet = keras.Model(inputs, outputs)
model_MobileNet.compile(loss="categorical_crossentropy",
    optimizer="rmsprop",
    metrics=["accuracy"])

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)

model_MobileNet.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

# Train the model
history = model_MobileNet.fit(
    x=X_train,y=y_train,
    epochs=150,
    validation_data=(X_test, y_test),
)

# Train the model
history = model_MobileNet.fit(
    x=X_train,y=y_train,
    epochs=150,
    validation_data=(X_test, y_test),
)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy-resnet")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss-resnet")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_MobileNet.predict(X_test)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for Resnet-combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-NIHSS_MobileNet {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_MobileNet.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve_NIHSS_MobileNet (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**Mobilnet**"""

from tensorflow.keras.applications import MobileNet
MobileNet = MobileNet(weights='imagenet', include_top=False, input_shape=(256, 256, 3))
for layer in MobileNet.layers:
  layer.trainable= False

# Create the ResNet model
base_model = keras.applications.MobileNet(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_MobileNet = keras.Model(inputs, outputs)

# Compile the model
model_MobileNet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_MobileNet.summary()

# Train the model
history = model_MobileNet1.fit(
    x=X_train,y=y_train,
    epochs=150,
    validation_data=(X_test, y_test),
)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy-resnet")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss-resnet")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_MobileNet1.predict(X_test)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for Resnet-combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-NIHSS_MobileNet {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_MobileNet1.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve_NIHSS_MobileNet (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**ResNet50**



"""

# Create the ResNet model
base_model = keras.applications.ResNet50(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

# Add the classification head
inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(3, activation="softmax")(x)
model_resnet = keras.Model(inputs, outputs)

# Compile the model
model_resnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model_resnet.summary()

# Train the model
history = model_resnet.fit(
    x=X_train,y=y_train,
    epochs=150,
    validation_data=(X_test, y_test),
)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy-resnet")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss-resnet")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_resnet.predict(X_test)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for Resnet-combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-resnet-combine {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_resnet.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve_resnet (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()

"""**ACL-ResNet-50**"""

import tensorflow as tf
from tensorflow import keras

# Create the ResNet model
base_model = keras.applications.ResNet50(include_top=False, weights="imagenet", input_shape=(256, 256, 3))
# Freeze the base model
base_model.trainable = False

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Create the ResNet model
base_model = keras.applications.ResNet50(
    include_top=False,
    weights="imagenet",
    input_shape=(256, 256, 3),
)

# Freeze the base model
base_model.trainable = False

inputs = keras.Input(shape=(256, 256, 3))
x = base_model(inputs, training=False)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(3, activation="softmax")(x)

model_resnet_morelayers = keras.Model(inputs=inputs, outputs=outputs)
model_resnet_morelayers.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

callbacks = [
    keras.callbacks.ModelCheckpoint(
    filepath="feature_extraction_with_data_augmentation.keras",
    save_best_only=True,
    monitor="val_loss")
    ]

# Train the model
history = model_resnet_morelayers.fit(
    x=X_train,y=y_train,
    epochs=150,
    validation_data=(X_test, y_test),
)

import matplotlib.pyplot as plt
accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "b", label="Training accuracy")
plt.plot(epochs, val_accuracy, "r", label="Validation accuracy")
plt.title("Training and validation accuracy-resnet")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "b", label="Training loss")
plt.plot(epochs, val_loss, "r", label="Validation loss")
plt.title("Training and validation loss-resnet")
plt.legend()
plt.show()

# Predict on test data
y_pred = model_resnet_morelayers.predict(X_test)

from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("Evaluation criteria for Resnet-combined")

# Calculate the confusion matrix for each label
cm = multilabel_confusion_matrix(y_test, y_pred.round())

# Extract TP, TN, FP, and FN from the confusion matrices
TP = cm[:, 1, 1]
TN = cm[:, 0, 0]
FP = cm[:, 0, 1]
FN = cm[:, 1, 0]

# Calculate sensitivity and specificity for each label
sensitivity = TP / (TP + FN) * 100
specificity = TN / (TN + FP) * 100

# Print the results
for i in range(len(sensitivity)):
    print("Label {}: Sensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))

# Print the confusion matrix for the first label
print("Confusion Matrix for Label 0:\n", cm[0])
# Plot the confusion matrices as heatmaps
for i in range(len(sensitivity)):
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots(figsize=(4, 4))
    sns.heatmap(cm[i], annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels, cbar=False, vmin=0, vmax=y_test[:, i].sum())
    plt.title("Confusion Matrix for Label-resnet-combine {}:\nSensitivity {:.2f}%, Specificity {:.2f}%".format(i, sensitivity[i], specificity[i]))
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Train your CNN model on the training data using VGG16

# Make predictions on the testing data
y_pred_prob = model_resnet_morelayers.predict(X_test)

# Compute the ROC curve and AUC score for each label
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_test.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and AUC score
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_pred_prob.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot the ROC curve for each label and micro-average
fig, ax = plt.subplots(figsize=(8, 8))
plt.plot([0, 1], [0, 1], 'k--')
for i in range(y_test.shape[1]):
    plt.plot(fpr[i], tpr[i], label='ROC curve for label {} (AUC = {:.2f})'.format(i, roc_auc[i]))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve_resnet (AUC = {:.2f})'.format(roc_auc["micro"]), linestyle=':', linewidth=4)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic (ROC) curve')
plt.legend(loc="lower right")
plt.show()